{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-01-17 14:02:59--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 46.6MB/s   in 55s    \n\n2020-01-17 14:03:54 (45.4 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Done\n"
                }
            ],
            "source": "!unzip -q Positive_tensors.zip \nprint(\"Done\")"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-01-17 14:06:14--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 43.9MB/s   in 43s    \n\n2020-01-17 14:06:58 (46.5 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\nDone\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip\nprint(\"Done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0MB 6.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nCollecting torch==1.4.0 (from torchvision)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 753.4MB 25kB/s s eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                        | 187.4MB 41.9MB/s eta 0:00:14\n\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nRequirement already satisfied: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.12.0)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.4.0 torchvision-0.5.0\nDone\n"
                }
            ],
            "source": "!pip install torchvision\nprint(\"Done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f71a5e00f30>"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/home/dsxuser/work\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n        \n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "288164ffca8241ec9ba9f260cfa366dd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\n# Type your code here\nmodel = models.resnet18(pretrained=True)\nprint(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\n\n\n# Type your code here\nfor param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "model.fc = nn.Linear(in_features=512, out_features=2)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\n# Type your code here\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "batch_size = 100\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size )\nval_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=batch_size)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Training set size:  30000\nValidation set size:  10000\n-----------------------------\nTraining Phase 1/300\nFinished in 5.834681272506714 (s)\n-----------------------------\nTraining Phase 2/300\nFinished in 5.951965093612671 (s)\n-----------------------------\nTraining Phase 3/300\nFinished in 5.920722246170044 (s)\n-----------------------------\nTraining Phase 4/300\nFinished in 5.915322542190552 (s)\n-----------------------------\nTraining Phase 5/300\nFinished in 5.908874273300171 (s)\n-----------------------------\nTraining Phase 6/300\nFinished in 5.894116163253784 (s)\n-----------------------------\nTraining Phase 7/300\nFinished in 6.077443599700928 (s)\n-----------------------------\nTraining Phase 8/300\nFinished in 5.973740339279175 (s)\n-----------------------------\nTraining Phase 9/300\nFinished in 5.859881639480591 (s)\n-----------------------------\nTraining Phase 10/300\nFinished in 5.960596561431885 (s)\n-----------------------------\nTraining Phase 11/300\nFinished in 5.907611846923828 (s)\n-----------------------------\nTraining Phase 12/300\nFinished in 5.917001247406006 (s)\n-----------------------------\nTraining Phase 13/300\nFinished in 5.981372833251953 (s)\n-----------------------------\nTraining Phase 14/300\nFinished in 5.902079105377197 (s)\n-----------------------------\nTraining Phase 15/300\nFinished in 5.9517982006073 (s)\n-----------------------------\nTraining Phase 16/300\nFinished in 5.824693918228149 (s)\n-----------------------------\nTraining Phase 17/300\nFinished in 5.89112114906311 (s)\n-----------------------------\nTraining Phase 18/300\nFinished in 5.891859769821167 (s)\n-----------------------------\nTraining Phase 19/300\nFinished in 6.11200737953186 (s)\n-----------------------------\nTraining Phase 20/300\nFinished in 5.801317453384399 (s)\n-----------------------------\nTraining Phase 21/300\nFinished in 5.872200012207031 (s)\n-----------------------------\nTraining Phase 22/300\nFinished in 5.9359400272369385 (s)\n-----------------------------\nTraining Phase 23/300\nFinished in 5.923269271850586 (s)\n-----------------------------\nTraining Phase 24/300\nFinished in 5.917081117630005 (s)\n-----------------------------\nTraining Phase 25/300\nFinished in 5.981318712234497 (s)\n-----------------------------\nTraining Phase 26/300\nFinished in 6.03926682472229 (s)\n-----------------------------\nTraining Phase 27/300\nFinished in 5.976207256317139 (s)\n-----------------------------\nTraining Phase 28/300\nFinished in 5.944674730300903 (s)\n-----------------------------\nTraining Phase 29/300\nFinished in 6.036716938018799 (s)\n-----------------------------\nTraining Phase 30/300\nFinished in 6.044224977493286 (s)\n-----------------------------\nTraining Phase 31/300\nFinished in 5.86328125 (s)\n-----------------------------\nTraining Phase 32/300\nFinished in 6.0612571239471436 (s)\n-----------------------------\nTraining Phase 33/300\nFinished in 5.871215343475342 (s)\n-----------------------------\nTraining Phase 34/300\nFinished in 5.762075901031494 (s)\n-----------------------------\nTraining Phase 35/300\nFinished in 5.9754602909088135 (s)\n-----------------------------\nTraining Phase 36/300\nFinished in 5.906489610671997 (s)\n-----------------------------\nTraining Phase 37/300\nFinished in 5.857804298400879 (s)\n-----------------------------\nTraining Phase 38/300\nFinished in 5.972690582275391 (s)\n-----------------------------\nTraining Phase 39/300\nFinished in 5.996138095855713 (s)\n-----------------------------\nTraining Phase 40/300\nFinished in 5.932781934738159 (s)\n-----------------------------\nTraining Phase 41/300\nFinished in 5.986451148986816 (s)\n-----------------------------\nTraining Phase 42/300\nFinished in 6.010627031326294 (s)\n-----------------------------\nTraining Phase 43/300\nFinished in 5.978432893753052 (s)\n-----------------------------\nTraining Phase 44/300\nFinished in 5.83283805847168 (s)\n-----------------------------\nTraining Phase 45/300\nFinished in 5.943041801452637 (s)\n-----------------------------\nTraining Phase 46/300\nFinished in 5.928829908370972 (s)\n-----------------------------\nTraining Phase 47/300\nFinished in 5.958359718322754 (s)\n-----------------------------\nTraining Phase 48/300\nFinished in 5.877230167388916 (s)\n-----------------------------\nTraining Phase 49/300\nFinished in 5.935025215148926 (s)\n-----------------------------\nTraining Phase 50/300\nFinished in 5.957045078277588 (s)\n-----------------------------\nTraining Phase 51/300\nFinished in 6.005453109741211 (s)\n-----------------------------\nTraining Phase 52/300\nFinished in 5.933629274368286 (s)\n-----------------------------\nTraining Phase 53/300\nFinished in 5.948871612548828 (s)\n-----------------------------\nTraining Phase 54/300\nFinished in 5.943526029586792 (s)\n-----------------------------\nTraining Phase 55/300\nFinished in 5.861106634140015 (s)\n-----------------------------\nTraining Phase 56/300\nFinished in 5.956284284591675 (s)\n-----------------------------\nTraining Phase 57/300\nFinished in 5.819375038146973 (s)\n-----------------------------\nTraining Phase 58/300\nFinished in 5.941882848739624 (s)\n-----------------------------\nTraining Phase 59/300\nFinished in 5.984010457992554 (s)\n-----------------------------\nTraining Phase 60/300\nFinished in 5.9210803508758545 (s)\n-----------------------------\nTraining Phase 61/300\nFinished in 5.947511434555054 (s)\n-----------------------------\nTraining Phase 62/300\nFinished in 5.900147199630737 (s)\n-----------------------------\nTraining Phase 63/300\nFinished in 6.022768497467041 (s)\n-----------------------------\nTraining Phase 64/300\nFinished in 5.977879285812378 (s)\n-----------------------------\nTraining Phase 65/300\nFinished in 5.884261846542358 (s)\n-----------------------------\nTraining Phase 66/300\nFinished in 5.982038736343384 (s)\n-----------------------------\nTraining Phase 67/300\nFinished in 6.119219541549683 (s)\n-----------------------------\nTraining Phase 68/300\nFinished in 6.005363464355469 (s)\n-----------------------------\nTraining Phase 69/300\nFinished in 5.986269235610962 (s)\n-----------------------------\nTraining Phase 70/300\nFinished in 5.91437840461731 (s)\n-----------------------------\nTraining Phase 71/300\nFinished in 6.243757724761963 (s)\n-----------------------------\nTraining Phase 72/300\nFinished in 5.908198833465576 (s)\n-----------------------------\nTraining Phase 73/300\nFinished in 5.997817277908325 (s)\n-----------------------------\nTraining Phase 74/300\nFinished in 5.892499208450317 (s)\n-----------------------------\nTraining Phase 75/300\nFinished in 5.9628517627716064 (s)\n-----------------------------\nTraining Phase 76/300\nFinished in 5.962317943572998 (s)\n-----------------------------\nTraining Phase 77/300\nFinished in 5.978632211685181 (s)\n-----------------------------\nTraining Phase 78/300\nFinished in 6.036589622497559 (s)\n-----------------------------\nTraining Phase 79/300\nFinished in 5.835500478744507 (s)\n-----------------------------\nTraining Phase 80/300\nFinished in 5.950148582458496 (s)\n-----------------------------\nTraining Phase 81/300\nFinished in 5.961557149887085 (s)\n-----------------------------\nTraining Phase 82/300\nFinished in 5.945685863494873 (s)\n-----------------------------\nTraining Phase 83/300\nFinished in 6.081401824951172 (s)\n-----------------------------\nTraining Phase 84/300\nFinished in 6.086055278778076 (s)\n-----------------------------\nTraining Phase 85/300\nFinished in 5.919538259506226 (s)\n-----------------------------\nTraining Phase 86/300\nFinished in 6.02647590637207 (s)\n-----------------------------\nTraining Phase 87/300\nFinished in 5.942727327346802 (s)\n-----------------------------\nTraining Phase 88/300\nFinished in 5.983739137649536 (s)\n-----------------------------\nTraining Phase 89/300\nFinished in 5.977293014526367 (s)\n-----------------------------\nTraining Phase 90/300\nFinished in 5.927226781845093 (s)\n-----------------------------\nTraining Phase 91/300\nFinished in 6.120385408401489 (s)\n-----------------------------\nTraining Phase 92/300\nFinished in 6.034075975418091 (s)\n-----------------------------\nTraining Phase 93/300\nFinished in 5.956335783004761 (s)\n-----------------------------\nTraining Phase 94/300\nFinished in 5.902828693389893 (s)\n-----------------------------\nTraining Phase 95/300\nFinished in 5.977096319198608 (s)\n-----------------------------\nTraining Phase 96/300\nFinished in 5.954038619995117 (s)\n-----------------------------\nTraining Phase 97/300\nFinished in 5.873424053192139 (s)\n-----------------------------\nTraining Phase 98/300\nFinished in 5.90370774269104 (s)\n-----------------------------\nTraining Phase 99/300\nFinished in 5.981743812561035 (s)\n-----------------------------\nTraining Phase 100/300\nFinished in 5.9193456172943115 (s)\n-----------------------------\nTraining Phase 101/300\nFinished in 6.030166149139404 (s)\n-----------------------------\nTraining Phase 102/300\nFinished in 6.055700302124023 (s)\n-----------------------------\nTraining Phase 103/300\nFinished in 6.145021200180054 (s)\n-----------------------------\nTraining Phase 104/300\nFinished in 5.977954626083374 (s)\n-----------------------------\nTraining Phase 105/300\nFinished in 5.959883213043213 (s)\n-----------------------------\nTraining Phase 106/300\nFinished in 5.9243433475494385 (s)\n-----------------------------\nTraining Phase 107/300\nFinished in 5.892704248428345 (s)\n-----------------------------\nTraining Phase 108/300\nFinished in 6.1107189655303955 (s)\n-----------------------------\nTraining Phase 109/300\nFinished in 5.975810289382935 (s)\n-----------------------------\nTraining Phase 110/300\nFinished in 6.052960634231567 (s)\n-----------------------------\nTraining Phase 111/300\nFinished in 5.898190259933472 (s)\n-----------------------------\nTraining Phase 112/300\nFinished in 6.001227140426636 (s)\n-----------------------------\nTraining Phase 113/300\nFinished in 5.941397666931152 (s)\n-----------------------------\nTraining Phase 114/300\nFinished in 6.017792701721191 (s)\n-----------------------------\nTraining Phase 115/300\nFinished in 5.999107599258423 (s)\n-----------------------------\nTraining Phase 116/300\nFinished in 5.976224660873413 (s)\n-----------------------------\nTraining Phase 117/300\nFinished in 5.991703748703003 (s)\n-----------------------------\nTraining Phase 118/300\nFinished in 6.393308877944946 (s)\n-----------------------------\nTraining Phase 119/300\nFinished in 6.065332651138306 (s)\n-----------------------------\nTraining Phase 120/300\nFinished in 6.127866983413696 (s)\n-----------------------------\nTraining Phase 121/300\nFinished in 5.966292142868042 (s)\n-----------------------------\nTraining Phase 122/300\nFinished in 6.201140642166138 (s)\n-----------------------------\nTraining Phase 123/300\nFinished in 5.946355104446411 (s)\n-----------------------------\nTraining Phase 124/300\nFinished in 6.023889064788818 (s)\n-----------------------------\nTraining Phase 125/300\nFinished in 5.896848440170288 (s)\n-----------------------------\nTraining Phase 126/300\nFinished in 6.0504889488220215 (s)\n-----------------------------\nTraining Phase 127/300\nFinished in 5.94146990776062 (s)\n-----------------------------\nTraining Phase 128/300\nFinished in 5.995694160461426 (s)\n-----------------------------\nTraining Phase 129/300\nFinished in 5.97707200050354 (s)\n-----------------------------\nTraining Phase 130/300\nFinished in 5.944338321685791 (s)\n-----------------------------\nTraining Phase 131/300\nFinished in 5.960355520248413 (s)\n-----------------------------\nTraining Phase 132/300\nFinished in 6.007826328277588 (s)\n-----------------------------\nTraining Phase 133/300\nFinished in 6.030541658401489 (s)\n-----------------------------\nTraining Phase 134/300\nFinished in 5.892945051193237 (s)\n-----------------------------\nTraining Phase 135/300\nFinished in 5.861136198043823 (s)\n-----------------------------\nTraining Phase 136/300\nFinished in 5.8902788162231445 (s)\n-----------------------------\nTraining Phase 137/300\nFinished in 5.963393688201904 (s)\n-----------------------------\nTraining Phase 138/300\nFinished in 5.879018306732178 (s)\n-----------------------------\nTraining Phase 139/300\nFinished in 6.0363006591796875 (s)\n-----------------------------\nTraining Phase 140/300\nFinished in 5.79984188079834 (s)\n-----------------------------\nTraining Phase 141/300\nFinished in 5.933005332946777 (s)\n-----------------------------\nTraining Phase 142/300\nFinished in 5.979405879974365 (s)\n-----------------------------\nTraining Phase 143/300\nFinished in 5.940112829208374 (s)\n-----------------------------\nTraining Phase 144/300\nFinished in 5.843383550643921 (s)\n-----------------------------\nTraining Phase 145/300\nFinished in 6.044255495071411 (s)\n-----------------------------\nTraining Phase 146/300\nFinished in 5.927474498748779 (s)\n-----------------------------\nTraining Phase 147/300\nFinished in 5.9554290771484375 (s)\n-----------------------------\nTraining Phase 148/300\nFinished in 5.904447317123413 (s)\n-----------------------------\nTraining Phase 149/300\nFinished in 5.934175252914429 (s)\n-----------------------------\nTraining Phase 150/300\nFinished in 5.905522108078003 (s)\n-----------------------------\nTraining Phase 151/300\nFinished in 5.88873553276062 (s)\n-----------------------------\nTraining Phase 152/300\nFinished in 5.894651889801025 (s)\n-----------------------------\nTraining Phase 153/300\nFinished in 5.955219745635986 (s)\n-----------------------------\nTraining Phase 154/300\nFinished in 5.972105503082275 (s)\n-----------------------------\nTraining Phase 155/300\nFinished in 5.94708251953125 (s)\n-----------------------------\nTraining Phase 156/300\nFinished in 6.053173303604126 (s)\n-----------------------------\nTraining Phase 157/300\nFinished in 5.916321516036987 (s)\n-----------------------------\nTraining Phase 158/300\nFinished in 5.97022271156311 (s)\n-----------------------------\nTraining Phase 159/300\nFinished in 5.961644172668457 (s)\n-----------------------------\nTraining Phase 160/300\nFinished in 5.910922527313232 (s)\n-----------------------------\nTraining Phase 161/300\nFinished in 5.940263509750366 (s)\n-----------------------------\nTraining Phase 162/300\nFinished in 5.950985431671143 (s)\n-----------------------------\nTraining Phase 163/300\nFinished in 5.963870286941528 (s)\n-----------------------------\nTraining Phase 164/300\nFinished in 5.9072425365448 (s)\n-----------------------------\nTraining Phase 165/300\nFinished in 5.957024574279785 (s)\n-----------------------------\nTraining Phase 166/300\nFinished in 5.93915057182312 (s)\n-----------------------------\nTraining Phase 167/300\nFinished in 5.867276906967163 (s)\n-----------------------------\nTraining Phase 168/300\nFinished in 5.912722826004028 (s)\n-----------------------------\nTraining Phase 169/300\nFinished in 6.002551317214966 (s)\n-----------------------------\nTraining Phase 170/300\nFinished in 6.024755954742432 (s)\n-----------------------------\nTraining Phase 171/300\nFinished in 5.89975118637085 (s)\n-----------------------------\nTraining Phase 172/300\nFinished in 5.9685218334198 (s)\n-----------------------------\nTraining Phase 173/300\nFinished in 6.079656362533569 (s)\n-----------------------------\nTraining Phase 174/300\nFinished in 5.908279657363892 (s)\n-----------------------------\nTraining Phase 175/300\nFinished in 5.992831468582153 (s)\n-----------------------------\nTraining Phase 176/300\nFinished in 6.006039142608643 (s)\n-----------------------------\nTraining Phase 177/300\nFinished in 6.11640477180481 (s)\n-----------------------------\nTraining Phase 178/300\nFinished in 6.017992973327637 (s)\n-----------------------------\nTraining Phase 179/300\nFinished in 6.012893438339233 (s)\n-----------------------------\nTraining Phase 180/300\nFinished in 6.105653285980225 (s)\n-----------------------------\nTraining Phase 181/300\nFinished in 5.965487480163574 (s)\n-----------------------------\nTraining Phase 182/300\nFinished in 6.053439140319824 (s)\n-----------------------------\nTraining Phase 183/300\nFinished in 5.958988428115845 (s)\n-----------------------------\nTraining Phase 184/300\nFinished in 6.063922166824341 (s)\n-----------------------------\nTraining Phase 185/300\nFinished in 5.9536871910095215 (s)\n-----------------------------\nTraining Phase 186/300\nFinished in 5.993462562561035 (s)\n-----------------------------\nTraining Phase 187/300\nFinished in 6.018493890762329 (s)\n-----------------------------\nTraining Phase 188/300\nFinished in 6.037269830703735 (s)\n-----------------------------\nTraining Phase 189/300\nFinished in 5.936579942703247 (s)\n-----------------------------\nTraining Phase 190/300\nFinished in 5.971534967422485 (s)\n-----------------------------\nTraining Phase 191/300\nFinished in 6.058517694473267 (s)\n-----------------------------\nTraining Phase 192/300\nFinished in 6.078015089035034 (s)\n-----------------------------\nTraining Phase 193/300\nFinished in 5.9155354499816895 (s)\n-----------------------------\nTraining Phase 194/300\nFinished in 6.012452602386475 (s)\n-----------------------------\nTraining Phase 195/300\nFinished in 5.929643630981445 (s)\n-----------------------------\nTraining Phase 196/300\nFinished in 5.957326889038086 (s)\n-----------------------------\nTraining Phase 197/300\nFinished in 6.011756420135498 (s)\n-----------------------------\nTraining Phase 198/300\nFinished in 5.990009546279907 (s)\n-----------------------------\nTraining Phase 199/300\nFinished in 6.019908666610718 (s)\n-----------------------------\nTraining Phase 200/300\nFinished in 5.875619649887085 (s)\n-----------------------------\nTraining Phase 201/300\nFinished in 6.020157814025879 (s)\n-----------------------------\nTraining Phase 202/300\nFinished in 6.009891033172607 (s)\n-----------------------------\nTraining Phase 203/300\nFinished in 5.872788190841675 (s)\n-----------------------------\nTraining Phase 204/300\nFinished in 6.055650472640991 (s)\n-----------------------------\nTraining Phase 205/300\nFinished in 5.924898147583008 (s)\n-----------------------------\nTraining Phase 206/300\nFinished in 5.906100034713745 (s)\n-----------------------------\nTraining Phase 207/300\nFinished in 5.965439319610596 (s)\n-----------------------------\nTraining Phase 208/300\nFinished in 5.9019293785095215 (s)\n-----------------------------\nTraining Phase 209/300\nFinished in 6.091949224472046 (s)\n-----------------------------\nTraining Phase 210/300\nFinished in 5.931220531463623 (s)\n-----------------------------\nTraining Phase 211/300\nFinished in 5.942385196685791 (s)\n-----------------------------\nTraining Phase 212/300\nFinished in 6.089094161987305 (s)\n-----------------------------\nTraining Phase 213/300\nFinished in 5.9465391635894775 (s)\n-----------------------------\nTraining Phase 214/300\nFinished in 6.159634828567505 (s)\n-----------------------------\nTraining Phase 215/300\nFinished in 5.97850489616394 (s)\n-----------------------------\nTraining Phase 216/300\nFinished in 5.929874897003174 (s)\n-----------------------------\nTraining Phase 217/300\nFinished in 6.007884979248047 (s)\n-----------------------------\nTraining Phase 218/300\nFinished in 5.818559885025024 (s)\n-----------------------------\nTraining Phase 219/300\nFinished in 6.045089244842529 (s)\n-----------------------------\nTraining Phase 220/300\nFinished in 5.923119306564331 (s)\n-----------------------------\nTraining Phase 221/300\nFinished in 6.035745620727539 (s)\n-----------------------------\nTraining Phase 222/300\nFinished in 5.9773595333099365 (s)\n-----------------------------\nTraining Phase 223/300\nFinished in 5.944251298904419 (s)\n-----------------------------\nTraining Phase 224/300\nFinished in 5.984071731567383 (s)\n-----------------------------\nTraining Phase 225/300\nFinished in 6.09190821647644 (s)\n-----------------------------\nTraining Phase 226/300\nFinished in 5.934859275817871 (s)\n-----------------------------\nTraining Phase 227/300\nFinished in 6.0483174324035645 (s)\n-----------------------------\nTraining Phase 228/300\nFinished in 6.5989274978637695 (s)\n-----------------------------\nTraining Phase 229/300\nFinished in 5.925483226776123 (s)\n-----------------------------\nTraining Phase 230/300\nFinished in 6.047512054443359 (s)\n-----------------------------\nTraining Phase 231/300\nFinished in 5.954392671585083 (s)\n-----------------------------\nTraining Phase 232/300\nFinished in 5.923432350158691 (s)\n-----------------------------\nTraining Phase 233/300\nFinished in 6.166585922241211 (s)\n-----------------------------\nTraining Phase 234/300\nFinished in 6.085103750228882 (s)\n-----------------------------\nTraining Phase 235/300\nFinished in 6.070337772369385 (s)\n-----------------------------\nTraining Phase 236/300\nFinished in 5.889143943786621 (s)\n-----------------------------\nTraining Phase 237/300\nFinished in 5.892695426940918 (s)\n-----------------------------\nTraining Phase 238/300\nFinished in 5.8658647537231445 (s)\n-----------------------------\nTraining Phase 239/300\nFinished in 6.046319484710693 (s)\n-----------------------------\nTraining Phase 240/300\nFinished in 6.09355616569519 (s)\n-----------------------------\nTraining Phase 241/300\nFinished in 6.040868043899536 (s)\n-----------------------------\nTraining Phase 242/300\nFinished in 5.986491680145264 (s)\n-----------------------------\nTraining Phase 243/300\nFinished in 5.995930433273315 (s)\n-----------------------------\nTraining Phase 244/300\nFinished in 5.99176287651062 (s)\n-----------------------------\nTraining Phase 245/300\nFinished in 6.114055395126343 (s)\n-----------------------------\nTraining Phase 246/300\nFinished in 5.928558588027954 (s)\n-----------------------------\nTraining Phase 247/300\nFinished in 6.0344507694244385 (s)\n-----------------------------\nTraining Phase 248/300\nFinished in 6.0176849365234375 (s)\n-----------------------------\nTraining Phase 249/300\nFinished in 6.113095045089722 (s)\n-----------------------------\nTraining Phase 250/300\nFinished in 5.955328941345215 (s)\n-----------------------------\nTraining Phase 251/300\nFinished in 6.015099763870239 (s)\n-----------------------------\nTraining Phase 252/300\nFinished in 5.9663074016571045 (s)\n-----------------------------\nTraining Phase 253/300\nFinished in 5.9711995124816895 (s)\n-----------------------------\nTraining Phase 254/300\nFinished in 6.158925294876099 (s)\n-----------------------------\nTraining Phase 255/300\nFinished in 6.002788782119751 (s)\n-----------------------------\nTraining Phase 256/300\nFinished in 6.013083457946777 (s)\n-----------------------------\nTraining Phase 257/300\nFinished in 6.1385743618011475 (s)\n-----------------------------\nTraining Phase 258/300\nFinished in 6.033663034439087 (s)\n-----------------------------\nTraining Phase 259/300\nFinished in 6.197714805603027 (s)\n-----------------------------\nTraining Phase 260/300\nFinished in 6.0070436000823975 (s)\n-----------------------------\nTraining Phase 261/300\nFinished in 5.972958564758301 (s)\n-----------------------------\nTraining Phase 262/300\nFinished in 6.027568340301514 (s)\n-----------------------------\nTraining Phase 263/300\nFinished in 5.8996641635894775 (s)\n-----------------------------\nTraining Phase 264/300\nFinished in 6.037597179412842 (s)\n-----------------------------\nTraining Phase 265/300\nFinished in 5.942453384399414 (s)\n-----------------------------\nTraining Phase 266/300\nFinished in 5.971746444702148 (s)\n-----------------------------\nTraining Phase 267/300\nFinished in 5.987360715866089 (s)\n-----------------------------\nTraining Phase 268/300\nFinished in 5.98888087272644 (s)\n-----------------------------\nTraining Phase 269/300\nFinished in 5.973454713821411 (s)\n-----------------------------\nTraining Phase 270/300\nFinished in 5.945494890213013 (s)\n-----------------------------\nTraining Phase 271/300\nFinished in 5.989122629165649 (s)\n-----------------------------\nTraining Phase 272/300\nFinished in 5.878382444381714 (s)\n-----------------------------\nTraining Phase 273/300\nFinished in 5.905514717102051 (s)\n-----------------------------\nTraining Phase 274/300\nFinished in 6.122967004776001 (s)\n-----------------------------\nTraining Phase 275/300\nFinished in 6.016828536987305 (s)\n-----------------------------\nTraining Phase 276/300\nFinished in 5.9426000118255615 (s)\n-----------------------------\nTraining Phase 277/300\nFinished in 5.872633695602417 (s)\n-----------------------------\nTraining Phase 278/300\nFinished in 5.954937696456909 (s)\n-----------------------------\nTraining Phase 279/300\nFinished in 5.923844575881958 (s)\n-----------------------------\nTraining Phase 280/300\nFinished in 6.152385950088501 (s)\n-----------------------------\nTraining Phase 281/300\nFinished in 5.957287788391113 (s)\n-----------------------------\nTraining Phase 282/300\nFinished in 5.947998523712158 (s)\n-----------------------------\nTraining Phase 283/300\nFinished in 5.885892868041992 (s)\n-----------------------------\nTraining Phase 284/300\nFinished in 5.8426313400268555 (s)\n-----------------------------\nTraining Phase 285/300\nFinished in 5.910246849060059 (s)\n-----------------------------\nTraining Phase 286/300\nFinished in 5.932521104812622 (s)\n-----------------------------\nTraining Phase 287/300\nFinished in 5.892756700515747 (s)\n-----------------------------\nTraining Phase 288/300\nFinished in 6.012500524520874 (s)\n-----------------------------\nTraining Phase 289/300\nFinished in 5.973767518997192 (s)\n-----------------------------\nTraining Phase 290/300\nFinished in 5.9800214767456055 (s)\n-----------------------------\nTraining Phase 291/300\nFinished in 6.022891521453857 (s)\n-----------------------------\nTraining Phase 292/300\nFinished in 5.989939451217651 (s)\n-----------------------------\nTraining Phase 293/300\nFinished in 5.979396820068359 (s)\n-----------------------------\nTraining Phase 294/300\nFinished in 6.054637432098389 (s)\n-----------------------------\nTraining Phase 295/300\nFinished in 5.918442249298096 (s)\n-----------------------------\nTraining Phase 296/300\nFinished in 6.024596691131592 (s)\n-----------------------------\nTraining Phase 297/300\nFinished in 5.937074661254883 (s)\n-----------------------------\nTraining Phase 298/300\nFinished in 5.976611852645874 (s)\n-----------------------------\nTraining Phase 299/300\nFinished in 5.949331045150757 (s)\n-----------------------------\nTraining Phase 300/300\nFinished in 5.964354038238525 (s)\n-----------------------------\nValidation Phase 1/300\nFinished in 6.004786252975464 (s)\n-----------------------------\nValidation Phase 2/300\nFinished in 5.851477146148682 (s)\n-----------------------------\nValidation Phase 3/300\nFinished in 5.869549036026001 (s)\n-----------------------------\nValidation Phase 4/300\nFinished in 5.993809461593628 (s)\n-----------------------------\nValidation Phase 5/300\nFinished in 5.917913913726807 (s)\n-----------------------------\nValidation Phase 6/300\nFinished in 6.008589029312134 (s)\n-----------------------------\nValidation Phase 7/300\nFinished in 5.962453365325928 (s)\n-----------------------------\nValidation Phase 8/300\nFinished in 5.938841342926025 (s)\n-----------------------------\nValidation Phase 9/300\nFinished in 5.897405385971069 (s)\n-----------------------------\nValidation Phase 10/300\nFinished in 5.908961296081543 (s)\n-----------------------------\nValidation Phase 11/300\nFinished in 5.845336198806763 (s)\n-----------------------------\nValidation Phase 12/300\nFinished in 5.835164308547974 (s)\n-----------------------------\nValidation Phase 13/300\nFinished in 5.8392333984375 (s)\n-----------------------------\nValidation Phase 14/300\nFinished in 5.8362627029418945 (s)\n-----------------------------\nValidation Phase 15/300\nFinished in 5.865381240844727 (s)\n-----------------------------\nValidation Phase 16/300\nFinished in 5.866461992263794 (s)\n-----------------------------\nValidation Phase 17/300\nFinished in 5.8458662033081055 (s)\n-----------------------------\nValidation Phase 18/300\nFinished in 5.825747489929199 (s)\n-----------------------------\nValidation Phase 19/300\nFinished in 5.855256795883179 (s)\n-----------------------------\nValidation Phase 20/300\nFinished in 5.885826349258423 (s)\n-----------------------------\nValidation Phase 21/300\nFinished in 5.897700071334839 (s)\n-----------------------------\nValidation Phase 22/300\nFinished in 5.811820983886719 (s)\n-----------------------------\nValidation Phase 23/300\nFinished in 5.899880409240723 (s)\n-----------------------------\nValidation Phase 24/300\nFinished in 5.842616319656372 (s)\n-----------------------------\nValidation Phase 25/300\nFinished in 5.8976287841796875 (s)\n-----------------------------\nValidation Phase 26/300\nFinished in 5.884525537490845 (s)\n-----------------------------\nValidation Phase 27/300\nFinished in 5.843984127044678 (s)\n-----------------------------\nValidation Phase 28/300\nFinished in 5.9749157428741455 (s)\n-----------------------------\nValidation Phase 29/300\nFinished in 5.89539909362793 (s)\n-----------------------------\nValidation Phase 30/300\nFinished in 5.829655885696411 (s)\n-----------------------------\nValidation Phase 31/300\nFinished in 6.046099424362183 (s)\n-----------------------------\nValidation Phase 32/300\nFinished in 5.847404956817627 (s)\n-----------------------------\nValidation Phase 33/300\nFinished in 5.84110951423645 (s)\n-----------------------------\nValidation Phase 34/300\nFinished in 5.879106283187866 (s)\n-----------------------------\nValidation Phase 35/300\nFinished in 5.884508848190308 (s)\n-----------------------------\nValidation Phase 36/300\nFinished in 5.937956809997559 (s)\n-----------------------------\nValidation Phase 37/300\nFinished in 5.909022092819214 (s)\n-----------------------------\nValidation Phase 38/300\nFinished in 5.961166858673096 (s)\n-----------------------------\nValidation Phase 39/300\nFinished in 6.015138149261475 (s)\n-----------------------------\nValidation Phase 40/300\nFinished in 5.888280868530273 (s)\n-----------------------------\nValidation Phase 41/300\nFinished in 5.843975067138672 (s)\n-----------------------------\nValidation Phase 42/300\nFinished in 5.833431243896484 (s)\n-----------------------------\nValidation Phase 43/300\nFinished in 5.834869146347046 (s)\n-----------------------------\nValidation Phase 44/300\nFinished in 5.9274537563323975 (s)\n-----------------------------\nValidation Phase 45/300\nFinished in 5.875134706497192 (s)\n-----------------------------\nValidation Phase 46/300\nFinished in 5.838461399078369 (s)\n-----------------------------\nValidation Phase 47/300\nFinished in 5.933469772338867 (s)\n-----------------------------\nValidation Phase 48/300\nFinished in 5.877726793289185 (s)\n-----------------------------\nValidation Phase 49/300\nFinished in 5.8755505084991455 (s)\n-----------------------------\nValidation Phase 50/300\nFinished in 5.941643953323364 (s)\n-----------------------------\nValidation Phase 51/300\nFinished in 5.9023215770721436 (s)\n-----------------------------\nValidation Phase 52/300\nFinished in 5.835937023162842 (s)\n-----------------------------\nValidation Phase 53/300\nFinished in 5.867419004440308 (s)\n-----------------------------\nValidation Phase 54/300\nFinished in 5.9126574993133545 (s)\n-----------------------------\nValidation Phase 55/300\nFinished in 5.882040023803711 (s)\n-----------------------------\nValidation Phase 56/300\nFinished in 5.95353627204895 (s)\n-----------------------------\nValidation Phase 57/300\nFinished in 5.854596376419067 (s)\n-----------------------------\nValidation Phase 58/300\nFinished in 5.902979135513306 (s)\n-----------------------------\nValidation Phase 59/300\nFinished in 5.890185117721558 (s)\n-----------------------------\nValidation Phase 60/300\nFinished in 5.853989839553833 (s)\n-----------------------------\nValidation Phase 61/300\nFinished in 5.910987377166748 (s)\n-----------------------------\nValidation Phase 62/300\nFinished in 5.8750550746917725 (s)\n-----------------------------\nValidation Phase 63/300\nFinished in 5.833488702774048 (s)\n-----------------------------\nValidation Phase 64/300\nFinished in 5.905731916427612 (s)\n-----------------------------\nValidation Phase 65/300\nFinished in 5.832090139389038 (s)\n-----------------------------\nValidation Phase 66/300\nFinished in 5.924138784408569 (s)\n-----------------------------\nValidation Phase 67/300\nFinished in 5.936899423599243 (s)\n-----------------------------\nValidation Phase 68/300\nFinished in 5.853722095489502 (s)\n-----------------------------\nValidation Phase 69/300\nFinished in 5.84083366394043 (s)\n-----------------------------\nValidation Phase 70/300\nFinished in 5.913666248321533 (s)\n-----------------------------\nValidation Phase 71/300\nFinished in 5.858391046524048 (s)\n-----------------------------\nValidation Phase 72/300\nFinished in 5.878395080566406 (s)\n-----------------------------\nValidation Phase 73/300\nFinished in 5.813129663467407 (s)\n-----------------------------\nValidation Phase 74/300\nFinished in 5.828265905380249 (s)\n-----------------------------\nValidation Phase 75/300\nFinished in 5.978303909301758 (s)\n-----------------------------\nValidation Phase 76/300\nFinished in 5.880386590957642 (s)\n-----------------------------\nValidation Phase 77/300\nFinished in 5.90176796913147 (s)\n-----------------------------\nValidation Phase 78/300\nFinished in 5.86683988571167 (s)\n-----------------------------\nValidation Phase 79/300\nFinished in 5.936476945877075 (s)\n-----------------------------\nValidation Phase 80/300\nFinished in 5.849395275115967 (s)\n-----------------------------\nValidation Phase 81/300\nFinished in 5.835263013839722 (s)\n-----------------------------\nValidation Phase 82/300\nFinished in 5.869517803192139 (s)\n-----------------------------\nValidation Phase 83/300\nFinished in 5.861412286758423 (s)\n-----------------------------\nValidation Phase 84/300\nFinished in 5.868499279022217 (s)\n-----------------------------\nValidation Phase 85/300\nFinished in 6.005151987075806 (s)\n-----------------------------\nValidation Phase 86/300\nFinished in 5.893484830856323 (s)\n-----------------------------\nValidation Phase 87/300\nFinished in 5.996690034866333 (s)\n-----------------------------\nValidation Phase 88/300\nFinished in 5.818336248397827 (s)\n-----------------------------\nValidation Phase 89/300\nFinished in 5.892377614974976 (s)\n-----------------------------\nValidation Phase 90/300\nFinished in 5.913967847824097 (s)\n-----------------------------\nValidation Phase 91/300\nFinished in 5.910115718841553 (s)\n-----------------------------\nValidation Phase 92/300\nFinished in 5.939081907272339 (s)\n-----------------------------\nValidation Phase 93/300\nFinished in 5.93405556678772 (s)\n-----------------------------\nValidation Phase 94/300\nFinished in 5.876942873001099 (s)\n-----------------------------\nValidation Phase 95/300\nFinished in 5.842577934265137 (s)\n-----------------------------\nValidation Phase 96/300\nFinished in 5.860176086425781 (s)\n-----------------------------\nValidation Phase 97/300\nFinished in 5.867618083953857 (s)\n-----------------------------\nValidation Phase 98/300\nFinished in 5.897328615188599 (s)\n-----------------------------\nValidation Phase 99/300\nFinished in 5.888970851898193 (s)\n-----------------------------\nValidation Phase 100/300\nFinished in 5.889997720718384 (s)\nFinished epoch 1 in 2569.032430410385 (s).\n"
                }
            ],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\naccuracy=0\ncorrect=0\nerror=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\nerr_xval = []\nerr_yval = []\n#n_epochs\n\nLoss=0\nprint(\"Training set size: \", N_train)\nprint(\"Validation set size: \", N_test)\n\nstart_time = time.time()\n\nfor epoch in range(n_epochs):    # loop for the number of epochs to train with\n    \n    \n    for i, (x, y) in enumerate(train_loader):\n        \n        print(\"-----------------------------\")\n        print('Training Phase {}/{}'.format(i+1, int(N_train/batch_size)))\n        istart_time = time.time()\n        \n#        model.train()   \n\n        model.train()\n    \n        #clear gradient \n        \n        optimizer.zero_grad()\n        \n        #make a prediction \n        z=model(x)\n        \n        # calculate loss\n        loss=criterion(z,y)   # z is predicted value - y is actual value\n        \n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        \n        loss_list.append(loss.data)\n        print(\"Finished in {} (s)\".format(time.time()-istart_time))\n        \n    correct=0  # initialize number of correct predicted values\n    \n    for i, (x_test, y_test) in enumerate(val_loader):\n        \n        print(\"-----------------------------\")\n        print('Validation Phase {}/{}'.format(i+1, int(N_test/batch_size)))\n        istart_time = time.time()\n        \n        # set model to eval \n        model.eval()\n        \n        #make a prediction \n        z = model(x_test)\n      \n        #find max \n        _, yhat = torch.max(z.data, 1)\n        \n        correct += (yhat == y_test).sum().item()\n        \n        #Calculate misclassified  samples in mini-batch \n        #hint +=(yhat==y_test).sum().item()\n        \n        print(\"Finished in {} (s)\".format(time.time()-istart_time))\n            \n    accuracy=correct/N_test\n    \n    print(\"Finished epoch {} in {} (s).\".format(epoch+1, time.time()-start_time))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9953"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Sample : 585; Expected Label: tensor([0]); Obtained Label: tensor([1])\nSample : 701; Expected Label: tensor([0]); Obtained Label: tensor([1])\nSample : 897; Expected Label: tensor([0]); Obtained Label: tensor([1])\nSample : 904; Expected Label: tensor([1]); Obtained Label: tensor([0])\n"
                }
            ],
            "source": "# I'm sure there's a way to get this without running the eval again but I can't figure it out\n# So I'll just do it this way.\n\ncount = 0\nmax_num_of_items = 4  # first four mis-classified samples\nvalidation_loader_batch_one = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n\nfor i, (x_test, y_test) in enumerate(validation_loader_batch_one):\n    # set model to eval\n    model.eval()\n    \n    # make a prediction\n    z = model(x_test)\n    \n    # find max\n    _, yhat = torch.max(z.data, 1)\n    \n    # print mis-classified samples\n    if yhat != y_test:\n        print(\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(i), str(y_test), str(yhat)))\n        count += 1\n        if count >= max_num_of_items:\n            break\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}